# -*- coding: utf-8 -*-

print("connected")

"""scraping_prototype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10yBoi7qMkm2395pz8L5iUa8ycdL-X97f

## Prototype of Scraping for Dataset 2
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
import asyncio
from playwright.async_api import async_playwright
import os
from google.cloud import storage

print("did imports")

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '../../../secrets/your_service_account_key.json'

# Function to upload a DataFrame directly to GCS (without saving locally)
def upload_dataframe_to_gcs(bucket_name, df, destination_blob_name):
    """Uploads a DataFrame directly to a GCS bucket without saving it locally."""
    
    # Create an in-memory buffer
    buffer = io.BytesIO()
    
    # Write the DataFrame to this buffer in CSV format
    df.to_csv(buffer, index=False)
    
    # Reset the buffer's position to the beginning
    buffer.seek(0)
    
    # Create a storage client
    storage_client = storage.Client()
    
    # Get the bucket and blob
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(destination_blob_name)
    
    # Upload the buffer's contents to GCS
    blob.upload_from_file(buffer, content_type='text/csv')
    
    print(f"DataFrame uploaded to {destination_blob_name} in bucket {bucket_name}.")

# Function to read a CSV directly from GCS into a pandas DataFrame
def read_csv_from_gcs(bucket_name, blob_name):
    """Reads a CSV file from GCS and returns it as a pandas DataFrame."""
    
    # Create a storage client
    storage_client = storage.Client()
    
    # Get the bucket and blob
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    
    # Download the blob's content as a string
    csv_data = blob.download_as_bytes()
    
    # Use io.BytesIO to load the bytes data into a pandas DataFrame
    data = io.BytesIO(csv_data)
    df = pd.read_csv(data)
    
    return df

# Base URL of the page (changing only the page number)
base_url = 'https://edit.tosdr.org/services?page='

# Create lists to hold the service names and ratings
services = []
ratings = []

# Loop through pages 1 to 415
for page_num in range(1, 416):
    # Construct the URL for the current page
    url = base_url + str(page_num)

    # Send a GET request to fetch the content of the page
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the relevant table containing the services
    services_table = soup.find('table')

    # Extract the service names and ratings
    for row in services_table.find_all('tr'):
        cols = row.find_all('td')
        if len(cols) > 1:
            service_name = cols[0].get_text(strip=True)
            rating = cols[1].get_text(strip=True)
            services.append(service_name)
            ratings.append(rating)

    # Random sleep to avoid being blocked
    time.sleep(random.uniform(1, 2))

# Create a DataFrame from the lists
df = pd.DataFrame({
    'Service': services,
    'Rating': ratings
})

#save the df as a csv
upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/services_and_ratings.csv')

# Display the DataFrame
print(df)

# Base URL of the services page (changing only the page number)
base_url = 'https://edit.tosdr.org/services?page='
annotate_base_url = 'https://edit.tosdr.org'

# Create a list to hold the annotate URLs
annotate_urls = []

# Loop through pages 1 to 5 (adjust for page range you want to scrape)
for page_num in range(1, 2):
    # Construct the URL for the current page
    url = base_url + str(page_num)

    # Send a GET request to fetch the content of the page
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the relevant table containing the services
    services_table = soup.find('table')

    # Extract the annotate URLs
    for row in services_table.find_all('tr'):
        annotate_button = row.find('a', string='Annotate')
        if annotate_button:
            annotate_link = annotate_button['href']
            full_annotate_url = annotate_base_url + annotate_link
            annotate_urls.append(full_annotate_url)

# Print the annotate URLs
for url in annotate_urls:
    print(url)

# URL for the AliExpress annotate page
annotate_url = 'https://edit.tosdr.org/services/1570/annotate'

# Send a GET request to fetch the content of the page
response = requests.get(annotate_url)
soup = BeautifulSoup(response.content, 'html.parser')

# URL for the AliExpress annotate page
annotate_url = 'https://edit.tosdr.org/services/1570/annotate'

# Send a GET request to fetch the content of the page
response = requests.get(annotate_url)
soup = BeautifulSoup(response.content, 'html.parser')

# Inspect the structure of the page by printing all 'div' elements
for div in soup.find_all('div'):
    print(div.prettify())

# Global variable to store the DataFrame
df = None

# Async function to scrape the page using Playwright
async def scrape_page():
    global df
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # Navigate to the page
        await page.goto('https://edit.tosdr.org/topics')

        # Wait for content to load
        await page.wait_for_selector('tr')

        # Extract the titles and links
        titles = await page.locator('tr td:nth-child(2) a').all_inner_texts()
        links = await page.locator('tr td:nth-child(2) a').evaluate_all('elements => elements.map(e => e.href)')

        # Create the DataFrame
        df = pd.DataFrame({'Title': titles, 'Link': links})

        # Close the browser
        await browser.close()

# Create and run the async function
asyncio.run(scrape_page())

#rename
df.rename(columns={'Title': 'Topic', 'Link': 'topic_link'}, inplace=True)

# Print the DataFrame
print(df)

# Save the DataFrame to a CSV file
upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/tosdr_topics.csv')

df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/tosdr_topics.csv')

# Async function to scrape cases for each topic
async def scrape_cases(topic, topic_link, page):
    await page.goto(topic_link)

    # Wait for the specific case list to load (targeting the specific div or section)
    await page.wait_for_selector('div.card-inline')

    # Extract the case titles and links (only within the specific section you're interested in)
    case_titles = await page.locator('div.card-inline td a').all_inner_texts()
    case_links = await page.locator('div.card-inline td a').evaluate_all('elements => elements.map(e => e.href)')

    # Return a list of cases, each associated with the topic
    cases = []
    for case_title, case_link in zip(case_titles, case_links):
        cases.append({
            'Topic': topic,
            'Topic Link': topic_link,
            'Case': case_title,
            'Case Link': case_link
        })
    return cases

# Async function to scrape the page using Playwright
async def scrape_page():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # Navigate to the topics page
        await page.goto('https://edit.tosdr.org/topics')
        await page.wait_for_selector('tr')

        # Extract the topic titles and links
        topics = await page.locator('tr td:nth-child(2) a').all_inner_texts()
        topic_links = await page.locator('tr td:nth-child(2) a').evaluate_all('elements => elements.map(e => e.href)')

        all_cases = []
        # Loop over each topic and get cases
        for topic, topic_link in zip(topics, topic_links):
            cases = await scrape_cases(topic, topic_link, page)
            all_cases.extend(cases)

        await browser.close()

        # Convert the list of cases to a DataFrame
        df = pd.DataFrame(all_cases)
        return df

# In a Jupyter notebook, use await directly instead of asyncio.run
df = asyncio.run(scrape_page())

# Display the DataFrame
print(df)

# Save the DataFrame to a CSV file
upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/topics_and_cases.csv')

#save info on classification and weight for each Topic Link

df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/tosdr_topics_and_cases.csv')

# Function to scrape classification and weight from the case link
def scrape_case_details(case_link):
    response = requests.get(case_link)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract Classification and Weight with fallback
    classification_tag = soup.find('p', string=lambda text: 'Classification' in text if text else False)
    weight_tag = soup.find('p', string=lambda text: 'Weight' in text if text else False)

    # Check for colon before splitting to avoid IndexError
    classification = classification_tag.get_text().split(":")[1].strip() if classification_tag and ":" in classification_tag.get_text() else None
    weight = weight_tag.get_text().split(":")[1].strip() if weight_tag and ":" in weight_tag.get_text() else None

    return classification, weight

# Main function to loop through case links
def scrape_page(df):
    all_data = []
    for _, row in df.iterrows():
        case_link = row['Case Link']
        classification, weight = scrape_case_details(case_link)

        # Append the results to the list
        all_data.append({
            'Case Link': case_link,
            'Classification': classification,
            'Weight': weight
        })

        # Add a random sleep between 1 and 3 seconds
        time.sleep(random.uniform(1, 2))

    return pd.DataFrame(all_data)

# Load your initial DataFrame
df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/tosdr_topics_and_cases.csv')

# Run the scraping function
df_cases = scrape_page(df)

# Display the DataFrame
print(df_cases)

# Save the DataFrame to a CSV file
upload_dataframe_to_gcs('legal-terms-data', df_cases, 'tosdr-data/raw/intermediate_data_files/tosdr_cases_with_classification_and_weight.csv')

df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/tosdr_topics_and_cases.csv')

# Function to scrape 'Service', 'Title', and 'Title Link' for all rows in a case (even if not approved)
def scrape_case_page(case_link):
    response = requests.get(case_link)
    soup = BeautifulSoup(response.content, 'html.parser')

    all_data = []

    # Loop through the rows in the table
    rows = soup.find_all('tr')
    for row in rows:
        status_tag = row.find('span', {'class': 'label-success'})
        
        # Check if the status is 'APPROVED'
        if status_tag and 'APPROVED' in status_tag.get_text(strip=True):
            status = 'APPROVED'
        else:
            status = 'NOT APPROVED'

        service = row.find('th')
        if service:
            service = service.get_text(strip=True)
        else:
            service = "N/A"  # Handle cases where 'th' is missing

        title_tag = row.find('td')
        if title_tag:
            a_tag = title_tag.find('a')
            if a_tag:
                title = a_tag.get_text(strip=True)
                title_link = 'https://edit.tosdr.org' + a_tag['href']  # Absolute URL for the title link
            else:
                title = "N/A"
                title_link = "N/A"
        else:
            title = "N/A"
            title_link = "N/A"

        all_data.append({
            'Case Link': case_link,  # Save Case Link instead of Topic Link
            'Service': service,
            'Title': title,
            'Title Link': title_link,
            'Status': status  # Add the new 'Status' column
        })

    return all_data

# Main function to loop through Case Links and scrape the necessary details
def scrape_all_rows(df):
    all_rows_data = []

    for _, row in df.iterrows():
        case_link = row['Case Link']

        # Scrape the case page for all entries
        all_data = scrape_case_page(case_link)
        all_rows_data.extend(all_data)

        # Add a random sleep between 1 and 3 seconds to avoid getting blocked
        time.sleep(random.uniform(1, 2))

    return pd.DataFrame(all_rows_data)

# Run the scraping function
df_all = scrape_all_rows(df)

# Display the resulting DataFrame
print(df_all)

# Optionally, save to a CSV
upload_dataframe_to_gcs('legal-terms-data', df_all, 'tosdr-data/raw/intermediate_data_files/all_cases.csv')

#loop through the link for each annotation, and just grab the name of the document that it comes from
df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/all_cases.csv')

# Function to scrape the source document name and link from the Title Link
def scrape_source_from_title_link(title_link):
    response = requests.get(title_link)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the blockquote, source document name, and link
    blockquote = soup.find('blockquote')
    if blockquote:
        footer = blockquote.find('footer')
        if footer:
            source_tag = footer.find('a')
            if source_tag:
                source_name = source_tag.get_text(strip=True)
                source_link = 'https://edit.tosdr.org' + source_tag['href']  # Add prefix to the source link
                return source_name, source_link

    return None, None

# Main function to loop through Title Links and update with source details
def update_with_source_details(df, save_interval=10):
    updated_data = []

    for i, row in df.iterrows():
        print(f"Processing row {i + 1}/{len(df)}...")  # Print the row number being processed

        title_link = row['Title Link']
        
        # Skip rows with missing or invalid URLs
        if pd.isna(title_link) or not title_link.startswith('http'):
            print(f"Skipping invalid URL: {title_link}")
            continue

        # Scrape the source document name and link
        source_name, source_link = scrape_source_from_title_link(title_link)

        # Add the source details to the existing row
        updated_row = {
            'Case Link': row['Case Link'],
            'Service': row['Service'],
            'Title': row['Title'],
            'Title Link': title_link,
            'Source Name': source_name,
            'Source Link': source_link
        }
        updated_data.append(updated_row)

        # Add a random sleep between 1 and 3 seconds to avoid being blocked
        time.sleep(random.uniform(0.1, 1))

        # Save progress every 'save_interval' rows
        if (i + 1) % save_interval == 0:
            df_temp = pd.DataFrame(updated_data)
            upload_dataframe_to_gcs('legal-terms-data', df_temp, 'tosdr-data/raw/intermediate_data_files/updated_cases_with_sources.csv')
            print(f"Saved progress at row {i + 1}.")

    # Final save when the loop is complete
    df_final = pd.DataFrame(updated_data)
    upload_dataframe_to_gcs('legal-terms-data', df_final, 'tosdr-data/raw/intermediate_data_files/updated_cases_with_sources.csv')
    print("Final save completed.")

    return df_final

df_updated = update_with_source_details(df, save_interval=10)

# Display the updated DataFrame with source information
print(df_updated)

#load in updated_cases_with_sources
df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/updated_cases_with_sources.csv')

# Create a new DataFrame with unique Source Links
unique_source_links = pd.DataFrame(df['Source Link'].unique(), columns=['Source Link'])

# Reset the index for the new DataFrame (optional)
unique_source_links.reset_index(drop=True, inplace=True)

upload_dataframe_to_gcs('legal-terms-data', unique_source_links, 'tosdr-data/raw/intermediate_data_files/unique_doc_links.csv')

df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/unique_doc_links.csv')

# Your login credentials
USERNAME = ""  # Replace with your username or email
PASSWORD = ""  # Replace with your password

# Async function to log in and scrape HTML content for a given URL
async def login_and_scrape(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  # Launch in headless mode
        page = await browser.new_page()

        # Navigate to the login page
        await page.goto("https://edit.tosdr.org/users/sign_in")

        # Fill in the login form
        await page.fill('input[name="user[email]"]', USERNAME)
        await page.fill('input[name="user[password]"]', PASSWORD)

        # Submit the login form
        await page.click('input[type="submit"]')

        # Wait for a specific element that indicates a successful login
        await page.wait_for_selector('a.navbar-tosdr-item[href="/cases"]')

        # Now navigate to the desired page after logging in
        await page.goto(url)

        # Extract the HTML content of the page
        html_content = await page.content()

        # Close the browser
        await browser.close()

        return html_content  # Return the HTML content

# Main function to loop over DataFrame and store results
# Main function to loop over DataFrame and store results
async def scrape_all_sources(df):
    # Create a new column for document text if it doesn't exist
    if 'document_text' not in df.columns:
        df['document_text'] = None

    for i, row in df.iterrows():
        print(f"Processing row {i + 1}/{len(df)}")
        source_link = row['Source Link']

        # Skip rows with missing or invalid URLs
        if pd.isna(source_link) or not source_link.startswith('http'):
            print(f"Skipping invalid URL: {source_link}")
            continue

        # Scrape the HTML content from the source link
        document_text = await login_and_scrape(source_link)

        # Store the scraped content in the DataFrame
        df.at[i, 'document_text'] = document_text

        # Save the DataFrame every 10 rows
        if (i + 1) % 10 == 0:
            upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/document_text_partial.csv')
            print(f"Saved partial DataFrame after processing {i + 1} rows.")

        # Add a random sleep between requests to avoid being blocked
        await asyncio.sleep(random.uniform(0.1, 2))

    # Save the final DataFrame after all rows have been processed
    upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/document_text_for_unique_links.csv')

#continue, starting at rows we don't have yet
df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/document_text_partial.csv')
# Keep only rows where 'document_text' is empty
df = df[df['document_text'].isna() | (df['document_text'] == '')]

# Print the length of the filtered DataFrame
print(len(df))

# Your login credentials
USERNAME = ""  # Replace with your username or email
PASSWORD = ""  # Replace with your password
print("credentials set")

# Async function to log in and scrape HTML content for a given URL
async def login_and_scrape(url):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  # Launch in headless mode
        page = await browser.new_page()

        # Navigate to the login page
        await page.goto("https://edit.tosdr.org/users/sign_in")

        # Fill in the login form
        await page.fill('input[name="user[email]"]', USERNAME)
        await page.fill('input[name="user[password]"]', PASSWORD)

        # Submit the login form
        await page.click('input[type="submit"]')

        # Wait for a specific element that indicates a successful login
        await page.wait_for_selector('a.navbar-tosdr-item[href="/cases"]')

        # Now navigate to the desired page after logging in
        await page.goto(url)

        # Extract the HTML content of the page
        html_content = await page.content()

        # Close the browser
        await browser.close()

        return html_content  # Return the HTML content

# Main function to loop over DataFrame and store results
# Main function to loop over DataFrame and store results
async def scrape_all_sources(df):
    # Create a new column for document text if it doesn't exist
    if 'document_text' not in df.columns:
        df['document_text'] = None

    for i, row in df.iterrows():
        print(f"Processing row {i + 1}/{len(df)}")
        source_link = row['Source Link']

        # Skip rows with missing or invalid URLs
        if pd.isna(source_link) or not source_link.startswith('http'):
            print(f"Skipping invalid URL: {source_link}")
            continue

        # Scrape the HTML content from the source link
        document_text = await login_and_scrape(source_link)

        # Store the scraped content in the DataFrame
        df.at[i, 'document_text'] = document_text

        # Save the DataFrame every 10 rows
        if (i + 1) % 10 == 0:
            upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/document_text_partial_try_again.csv')
            print(f"Saved partial DataFrame after processing {i + 1} rows.")

        # Add a random sleep between requests to avoid being blocked
        await asyncio.sleep(random.uniform(0.1, 2))

    # Save the final DataFrame after all rows have been processed
    upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/document_text_for_unique_links_try_again.csv')

# Assuming df is already loaded in your environment
# Run the scraping function on your already loaded DataFrame
asyncio.run(scrape_all_sources(df))

#append the two csvs together.
file1 = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/document_text_partial.csv')
file2 = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/document_text_for_unique_links_try_again.csv')

# Merge the two DataFrames on column 'A', using an outer join
combined = pd.merge(file1, file2, on='Source Link', how='outer', suffixes=('_file1', '_file2'))

# Combine the B columns, filling missing values from both files
combined['document_text'] = combined['document_text_file1'].combine_first(combined['document_text_file2'])

# Select the final columns
final_output = combined[['Source Link', 'document_text']]

# Save the result to a new CSV file
upload_dataframe_to_gcs('legal-terms-data', final_output, 'tosdr-data/raw/intermediate_data_files/combined_full_html_content.csv')

df = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/combined_full_html_content.csv')

# Ensure that the columns are of type string
df['document_text'] = df['document_text'].astype(str)
df['Source Link'] = df['Source Link'].astype(str)

# Extract the document ID by splitting the 'Source Link' and taking the second part (index 1)
df['doc_id'] = df['Source Link'].str.split("doc_").str[1]  # Use .str[1] to get everything after 'doc_'


# Create the 'start' column by concatenating 'hi' with the 'doc_id'
df['start'] = '/documents/' + df['doc_id'].astype(str)  # Concatenate 'hi' with the string representation of 'doc_id'
df['end'] = 'data-document-id="' + df['doc_id'].astype(str)

# Create 'document_text_shortened' column based on 'start' and 'end'
def extract_text(row):
    start_index = row['document_text'].find(row['start']) + len(row['start'])
    end_index = row['document_text'].find(row['end'])
    
    if start_index != -1 and end_index != -1 and start_index < end_index:
        return row['document_text'][start_index:end_index].strip()
    return None  # Return None if start or end is not found

# Apply the function to each row in the DataFrame
df['document_text_shortened'] = df.apply(extract_text, axis=1)
df  = df[['Source Link', 'document_text_shortened']]
upload_dataframe_to_gcs('legal-terms-data', df, 'tosdr-data/raw/intermediate_data_files/document_text_for_unique_links.csv')

# do all the merges to put all of the csvs created in this doc together.

df1 = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/tosdr_topics_and_cases.csv')
df2 = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/tosdr_cases_with_classification_and_weight.csv')

#merge on the column Case Link
merged_df = pd.merge(df1, df2, on='Case Link', how='inner')
df3a = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/updated_cases_with_sources.csv')
df3b = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/all_cases.csv')


#make sure title link is a string in both - this is the different part.
df3a['Title Link'] = df3a['Title Link'].astype(str)
df3b['Title Link'] = df3b['Title Link'].astype(str)

df3 = pd.merge(df3a, df3b[['Status', 'Title Link']], on='Title Link' , how='left')
print(df3)
# Perform a left merge to retain all rows from df3
final_df = pd.merge(merged_df, df3, on='Case Link', how='right')

#merge in the ratings
df4 = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/services_and_ratings.csv')
final_df = pd.merge(final_df, df4[['Service', 'Rating']], on='Service', how='left')
print(final_df['Source Link'].count())

#make sure source link is a string in both cases
final_df['Source Link'] = final_df['Source Link'].astype(str)
print(final_df['Source Link'].count())

df5 = read_csv_from_gcs('legal-terms-data', 'tosdr-data/raw/intermediate_data_files/document_text_for_unique_links.csv')
df5['Source Link'] = df5['Source Link'].astype(str)

extra_final_df = pd.merge(final_df, df5[['Source Link', 'document_text_shortened']], on='Source Link', how='left')
print(extra_final_df['document_text_shortened'].count())
upload_dataframe_to_gcs('legal-terms-data', extra_final_df, 'tosdr-data/raw/final_output.csv')
