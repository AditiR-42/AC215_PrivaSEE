# -*- coding: utf-8 -*-
"""scraping_prototype.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10yBoi7qMkm2395pz8L5iUa8ycdL-X97f

## Prototype of Scraping for Dataset 2
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# Base URL of the page (changing only the page number)
base_url = 'https://edit.tosdr.org/services?page='

# Create lists to hold the service names and ratings
services = []
ratings = []

# Loop through pages 1 to 415
for page_num in range(1, 416):
    # Construct the URL for the current page
    url = base_url + str(page_num)

    # Send a GET request to fetch the content of the page
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the relevant table containing the services
    services_table = soup.find('table')

    # Extract the service names and ratings
    for row in services_table.find_all('tr'):
        cols = row.find_all('td')
        if len(cols) > 1:
            service_name = cols[0].get_text(strip=True)
            rating = cols[1].get_text(strip=True)
            services.append(service_name)
            ratings.append(rating)

    # Random sleep to avoid being blocked
    time.sleep(random.uniform(1, 10))

# Create a DataFrame from the lists
df = pd.DataFrame({
    'Service': services,
    'Rating': ratings
})

#save the df as a csv
df.to_csv('services_and_ratings.csv')

# Display the DataFrame
print(df)

print("hi")

import requests
from bs4 import BeautifulSoup

# Base URL of the services page (changing only the page number)
base_url = 'https://edit.tosdr.org/services?page='
annotate_base_url = 'https://edit.tosdr.org'

# Create a list to hold the annotate URLs
annotate_urls = []

# Loop through pages 1 to 5 (adjust for page range you want to scrape)
for page_num in range(1, 2):
    # Construct the URL for the current page
    url = base_url + str(page_num)

    # Send a GET request to fetch the content of the page
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the relevant table containing the services
    services_table = soup.find('table')

    # Extract the annotate URLs
    for row in services_table.find_all('tr'):
        annotate_button = row.find('a', string='Annotate')
        if annotate_button:
            annotate_link = annotate_button['href']
            full_annotate_url = annotate_base_url + annotate_link
            annotate_urls.append(full_annotate_url)

# Print the annotate URLs
for url in annotate_urls:
    print(url)

import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL for the AliExpress annotate page
annotate_url = 'https://edit.tosdr.org/services/1570/annotate'

# Send a GET request to fetch the content of the page
response = requests.get(annotate_url)
soup = BeautifulSoup(response.content, 'html.parser')
soup

import requests
from bs4 import BeautifulSoup

# URL for the AliExpress annotate page
annotate_url = 'https://edit.tosdr.org/services/1570/annotate'

# Send a GET request to fetch the content of the page
response = requests.get(annotate_url)
soup = BeautifulSoup(response.content, 'html.parser')

# Inspect the structure of the page by printing all 'div' elements
for div in soup.find_all('div'):
    print(div.prettify())

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import pandas as pd
import time

# Path to your ChromeDriver
driver_path = '/path/to/chromedriver'

# Set up the ChromeDriver service
service = Service(driver_path)
driver = webdriver.Chrome(service=service)

# URL for the AliExpress annotate page
annotate_url = 'https://edit.tosdr.org/services/1570/annotate'

# Navigate to the page
driver.get(annotate_url)

# Let the page load
time.sleep(3)

# Get page source after JavaScript has rendered
page_source = driver.page_source

# Parse the page with BeautifulSoup
soup = BeautifulSoup(page_source, 'html.parser')

# Close the browser
driver.quit()

# Now, let's try to find the highlighted text and annotations
highlighted_texts = []
annotations = []

for annotation_item in soup.find_all('div', class_='annotation'):
    highlighted = annotation_item.find('mark')  # Try 'mark' or another tag for highlighted text
    if highlighted:
        highlighted_texts.append(highlighted.get_text(strip=True))

    annotation_text = annotation_item.find('div', class_='annotation-text').get_text(strip=True)
    annotations.append(annotation_text)

# Create a DataFrame to hold the results
df = pd.DataFrame({
    'Highlighted Text': highlighted_texts,
    'Annotation': annotations
})

# Display the DataFrame
print(df)

#trying this a different way: going through cases.

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import time

# Path to your ChromeDriver
driver_path = '/path/to/chromedriver'

# Set up the ChromeDriver service
service = Service(driver_path)
driver = webdriver.Chrome(service=service)

# URL for the page to scrape
url = 'https://edit.tosdr.org/cases'

# Navigate to the page
driver.get(url)

# Let the page load (adjust the time if needed)
time.sleep(3)

# Get the page source after JavaScript has rendered
page_source = driver.page_source

# Parse the page with BeautifulSoup
soup = BeautifulSoup(page_source, 'html.parser')

# Close the browser
driver.quit()

# Find and print the titles under the "Title" column
for row in soup.find_all('tr'):
    cols = row.find_all('td')
    if len(cols) > 0:
        title = cols[0].get_text(strip=True)
        print(title)

import asyncio
from playwright.async_api import async_playwright
import pandas as pd

# Global variable to store the DataFrame
df = None

# Async function to scrape the page using Playwright
async def scrape_page():
    global df
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # Navigate to the page
        await page.goto('https://edit.tosdr.org/topics')

        # Wait for content to load
        await page.wait_for_selector('tr')

        # Extract the titles and links
        titles = await page.locator('tr td:nth-child(2) a').all_inner_texts()
        links = await page.locator('tr td:nth-child(2) a').evaluate_all('elements => elements.map(e => e.href)')

        # Create the DataFrame
        df = pd.DataFrame({'Title': titles, 'Link': links})

        # Close the browser
        await browser.close()

# Create and run the async function
await scrape_page()

#rename
df.rename(columns={'Title': 'Topic', 'Link': 'topic_link'}, inplace=True)

# Print the DataFrame
print(df)

# Save the DataFrame to a CSV file
df.to_csv('tosdr_topics.csv', index=False)

df = pd.read_csv('tosdr_topics.csv')
df



import asyncio
from playwright.async_api import async_playwright
import pandas as pd

# Async function to scrape cases for each topic
async def scrape_cases(topic, topic_link, page):
    await page.goto(topic_link)

    # Wait for the specific case list to load (targeting the specific div or section)
    await page.wait_for_selector('div.card-inline')

    # Extract the case titles and links (only within the specific section you're interested in)
    case_titles = await page.locator('div.card-inline td a').all_inner_texts()
    case_links = await page.locator('div.card-inline td a').evaluate_all('elements => elements.map(e => e.href)')

    # Return a list of cases, each associated with the topic
    cases = []
    for case_title, case_link in zip(case_titles, case_links):
        cases.append({
            'Topic': topic,
            'Topic Link': topic_link,
            'Case': case_title,
            'Case Link': case_link
        })
    return cases

# Async function to scrape the page using Playwright
async def scrape_page():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # Navigate to the topics page
        await page.goto('https://edit.tosdr.org/topics')
        await page.wait_for_selector('tr')

        # Extract the topic titles and links
        topics = await page.locator('tr td:nth-child(2) a').all_inner_texts()
        topic_links = await page.locator('tr td:nth-child(2) a').evaluate_all('elements => elements.map(e => e.href)')

        all_cases = []
        # Loop over each topic and get cases
        for topic, topic_link in zip(topics, topic_links):
            cases = await scrape_cases(topic, topic_link, page)
            all_cases.extend(cases)

        await browser.close()

        # Convert the list of cases to a DataFrame
        df = pd.DataFrame(all_cases)
        return df

# In a Jupyter notebook, use await directly instead of asyncio.run
df = await scrape_page()

# Display the DataFrame
print(df)

# Save the DataFrame to a CSV file
df.to_csv('tosdr_topics_and_cases.csv', index=False)

#save info on classification and weight for each Topic Link

df = pd.read_csv('tosdr_topics_and_cases.csv')
df

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# Function to scrape classification and weight from the case link
def scrape_case_details(case_link):
    response = requests.get(case_link)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract Classification and Weight with fallback
    classification_tag = soup.find('p', string=lambda text: 'Classification' in text if text else False)
    weight_tag = soup.find('p', string=lambda text: 'Weight' in text if text else False)

    # Check for colon before splitting to avoid IndexError
    classification = classification_tag.get_text().split(":")[1].strip() if classification_tag and ":" in classification_tag.get_text() else None
    weight = weight_tag.get_text().split(":")[1].strip() if weight_tag and ":" in weight_tag.get_text() else None

    return classification, weight

# Main function to loop through case links
def scrape_page(df):
    all_data = []
    for _, row in df.iterrows():
        case_link = row['Case Link']
        classification, weight = scrape_case_details(case_link)

        # Append the results to the list
        all_data.append({
            'Case Link': case_link,
            'Classification': classification,
            'Weight': weight
        })

        # Add a random sleep between 1 and 3 seconds
        time.sleep(random.uniform(1, 3))

    return pd.DataFrame(all_data)

# Load your initial DataFrame
df = pd.read_csv('tosdr_topics_and_cases.csv')

# Run the scraping function
df_cases = scrape_page(df)

# Display the DataFrame
print(df_cases)

# Save the DataFrame to a CSV file
df_cases.to_csv('tosdr_cases_with_classification_and_weight.csv', index=False)

df = pd.read_csv('tosdr_topics_and_cases.csv')
df

#smaller df to test the following with
df_small_test = df.head()
df_small_test

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# Function to scrape 'Service', 'Title', and 'Title Link' for approved rows in a case
def scrape_case_page(case_link):
    response = requests.get(case_link)
    soup = BeautifulSoup(response.content, 'html.parser')

    approved_data = []

    # Loop through the rows in the table
    rows = soup.find_all('tr', {'data-classification': 'bad'})
    for row in rows:
        status = row.find('span', {'class': 'label-success'})

        if status and 'APPROVED' in status.get_text(strip=True):
            service = row.find('th').get_text(strip=True)
            title_tag = row.find('td').find('a')
            title = title_tag.get_text(strip=True)
            title_link = 'https://edit.tosdr.org' + title_tag['href']  # Absolute URL for the title link

            approved_data.append({
                'Case Link': case_link,  # Save Case Link instead of Topic Link
                'Service': service,
                'Title': title,
                'Title Link': title_link
            })

    return approved_data

# Main function to loop through Case Links and scrape the necessary details
def scrape_approved_rows(df):
    all_approved_data = []

    for _, row in df.iterrows():
        case_link = row['Case Link']

        # Scrape the case page for approved entries
        approved_data = scrape_case_page(case_link)
        all_approved_data.extend(approved_data)

        # Add a random sleep between 1 and 3 seconds to avoid getting blocked
        time.sleep(random.uniform(1, 3))

    return pd.DataFrame(all_approved_data)

# Run the scraping function
df_approved = scrape_approved_rows(df)

# Display the resulting DataFrame
print(df_approved)

# Optionally, save to a CSV
df_approved.to_csv('approved_cases.csv', index=False)

#loop through the link for each approved annotation, and just grab the name of the document that it comes from
df = pd.read_csv('approved_cases.csv')
df

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# Function to scrape the source document name and link from the Title Link
def scrape_source_from_title_link(title_link):
    response = requests.get(title_link)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the blockquote, source document name, and link
    blockquote = soup.find('blockquote')
    if blockquote:
        footer = blockquote.find('footer')
        if footer:
            source_tag = footer.find('a')
            if source_tag:
                source_name = source_tag.get_text(strip=True)
                source_link = 'https://edit.tosdr.org' + source_tag['href']  # Add prefix to the source link
                return source_name, source_link

    return None, None

# Main function to loop through Title Links and update with source details
def update_with_source_details(df, save_interval=10):
    updated_data = []

    for i, row in df.iterrows():
        print(f"Processing row {i + 1}/{len(df)}...")  # Print the row number being processed

        title_link = row['Title Link']

        # Scrape the source document name and link
        source_name, source_link = scrape_source_from_title_link(title_link)

        # Add the source details to the existing row
        updated_row = {
            'Case Link': row['Case Link'],
            'Service': row['Service'],
            'Title': row['Title'],
            'Title Link': title_link,
            'Source Name': source_name,
            'Source Link': source_link
        }
        updated_data.append(updated_row)

        # Add a random sleep between 1 and 3 seconds to avoid being blocked
        time.sleep(random.uniform(1, 2))

        # Save progress every 'save_interval' rows
        if (i + 1) % save_interval == 0:
            df_temp = pd.DataFrame(updated_data)
            df_temp.to_csv('updated_cases_with_sources.csv', index=False)
            print(f"Saved progress at row {i + 1}.")

    # Final save when the loop is complete
    df_final = pd.DataFrame(updated_data)
    df_final.to_csv('updated_cases_with_sources.csv', index=False)
    print("Final save completed.")

    return df_final

# Assuming df_approved is the DataFrame already loaded into your Python environment
df_updated = update_with_source_details(df, save_interval=10)

# Display the updated DataFrame with source information
print(df_updated)



"""## do all the merges to put all of the csvs created in this doc together."""

df1 = pd.read_csv('tosdr_topics_and_cases.csv')
df2 = pd.read_csv('tosdr_cases_with_classification_and_weight.csv')

#merge on the column Case Link
merged_df = pd.merge(df1, df2, on='Case Link', how='inner')

merged_df

df3 = pd.read_csv('updated_cases_with_sources.csv')
df3

# Perform a left merge to retain all rows from df3
final_df = pd.merge(merged_df, df3, on='Case Link', how='right')
final_df

#merge in the ratings
df4 = pd.read_csv('services_and_ratings.csv')
df4

extra_final_df = pd.merge(final_df, df4[['Service', 'Rating']], on='Service', how='left')
extra_final_df

extra_final_df.to_csv('final_output.csv', index=False)

#next step is to figure out pattern in why some annotations don't get picked up. but merge things into a more understandable document first.